import ollama
from ..config import OLLAMA_MODEL, OLLAMA_TEMPERATURE


def generate_local_answer(prompt):
    """
    Call Ollama local model to generate answer
    :param prompt: Prompt
    :return: Answer generated by model
    """
    from ..config import SYSTEM_ROLE
    try:
        response = ollama.chat(
            model=OLLAMA_MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_ROLE},
                {"role": "user", "content": prompt}
            ],
            options={"temperature": OLLAMA_TEMPERATURE}
        )
        return response.message.content
    except Exception as e:
        return f"Error calling local Ollama model: {e}"